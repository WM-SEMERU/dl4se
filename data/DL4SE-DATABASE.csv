Title,Venue,Year published,Type of Paper,SE Problem,Data-Scale,SE Data,[data] Code,[data] Repo Metadata,[data] I/O Examples,Data Preprocessing,[pre] Tokenization,[pre] Neural Embedding,[pre] I/O Vectors,Learning Type,Architectures,Tuning,Learning Algorithm,Loss Function,Metrics,[metric] Coverage & Proportions,[metric] Ranking,[metric] Alignment Score,[metric] Approx Error,[metric] ROC or AUC,[metric] Accuracy,[metric] Precision,[metric] Recall,[metric] F1,Novel Automation,Exploratory Data Analysis,Occam's Razor,Missing Learning Elements,Combat Overfitting,[over] hyper-tuning,[over] custom,[over] Data Augmentation,[over] Data Balancing,[over] Cross-Validation,[over] Early Stop,[over] Data Cleaning,[over] Dropout,[over] L# Regularization,[over] Large Dataset,Replicability,Reproducible,[repro] Learning Algorithm,[repro] Loss Function,[repro] Hyperparameters,[repro] TVT Details Missing,[repro] Embedding Details,[repro] Regularization ,[repro] EDA,[repro] Extraction Details,[repro] Missing Filtering Details,[repro] No Repo
A convolutional attention network for extreme summarization of source code,ICML,2016,Technical,Source Code Summarization,Tens,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,supervised,CNN,Bayesian Opt,Gradient Descent,Max Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,repro-regularization,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
A deep learning approach to identifying source code in images and video,MSR,2018,Technical,Image2Structure,Thousands,Vision,non-Code,non-RepoMeta,non-I/O,Video Frame,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,CNN,No Tunning,Gradient Descent,Max Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,non-F1,Solving Previously Unsolvable Problems,EDA,no-Razor,no-miss-learning,over-dual,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
A deep learning model for estimating story points,TSE,2018,Journal,non-Code Related,Thousands,Repo Metadata,non-Code,Issues,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,RNN,Grid Search,Gradient Descent,Negative Log Likelihood,Dual-metric,non-cover,non-rank,non-alignment,approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Replacing Expertise,EDA,Razor,no-miss-learning,over-dual,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
A Neural Framework for Retrieval and Summarization of Source Code,ASE,2018,Short Paper,Source Code Retrieval & Traceability,Thousands,Natural,Snippets,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,supervised,Autoencoder,Grid Search,VAE,Max Log Likelihood,Dual-metric,non-cover,MRR,alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,no-EDA,Razor,no-miss-learning,over-trial,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,early-stop,non-o-cleaning,over-dropout,over-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,NoRepo
An Empirical Investigation into Learning Bug-fixing Patches in the Wild via Neural Machine Translation,ASE,2018,Short Paper,Program Repair,Thousands,Code,Methods,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,Grid Search,Gradient Descent,Negative Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,non-Recall,non-F1,Replacing Expertise,EDA,Razor,no-miss-learning,over-trial,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,early-stop,non-o-cleaning,over-dropout,non-regularization,over-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Applying Deep Learning Based Automatic Bug Triager to Industrial Projects,FSE,2017,Short Paper,Bug-Fixing Process,Thousands,Bug Report,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,CNN,No Tunning,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,no-Razor,miss-learning,over-dual,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,over-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,repro-loss,Hyper,tvt,non-repro-embed,non-repro-regul,repro-EDA,ExDe,repro-Filter,NoRepo
Are deep neural networks the best choice for modeling source code?,FSE,2017,Journal,Souce Code Generation,Thousands,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,unsupervised,RNN,Grid Search,Gradient Descent,Cross-entropy,Dual-metric,non-cover,MRR,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Understanding of the Topic,EDA,Razor,miss-learning,over-dual,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,ExDe,repro-Filter,NoRepo
Automatic feature learning for predicting vulnerable software components,TSE,2018,Journal,Software Security,Thousands,Code,Class,non-RepoMeta,non-I/O,Lookup Tables,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,RNN,Grid Search,Gradient Descent,Negative Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,RocOrAUC,non-Acc,Prec,Recall,F1,Advanced Architecture / Novelty,EDA,Razor,no-miss-learning,dropout,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Automatic Program Synthesis of Long Programs with a Learned Garbage Collector,NIPS,2018,Journal,Program Synthesis,Thousands,I/O,non-Code,non-RepoMeta,I/O,One Hot,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,FNN,No Tunning,Gradient Descent,Cross-entropy,Distance-Based,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,EDA,no-Razor,no-miss-learning,cleaning,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Automatically generating commit messages from diffs using neural machine translation,ASE,2017,Journal,non-Code Related,Thousands,Natural,non-Code,Diffs,non-I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,Cross-entropy,Distance-Based,non-cover,non-rank,alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,no-Razor,no-miss-learning,ensemble,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Automatically Learning Semantic Features for Defect Prediction,ICSE,2016,Technical,Reliability & Defect Prediction,Thousands,Code,Analytics-Graphical,non-RepoMeta,non-I/O,One Hot,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,semi-supervised,Dual-arch,Random Search,Gradient Descent,Max Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Advanced Architecture / Novelty,EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,LearningAlg,non-loss,Hyper,tvt,non-repro-embed,repro-regularization,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Automating Intention Mining,TSE,2018,Journal,Developer Intention Mining,Thousands,Repo Metadata,non-Code,Issues,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,CNN,Custom-Manual-Empirical,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,batch-norm,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,k-fold,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Bayesian specification learning for finding API usage errors,FSE,2017,Journal,Bug-Fixing Process,Hundreds,LDA,Program,non-RepoMeta,non-I/O,One Hot,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,no-Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,NoRepo
Automatic Text Input Generation for Mobile Testing,ICSE,2017,Journal,Software Testing,Thousands,Vision,non-Code,Diffs,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,N/A-Loss,Coverage & Proportions,Coverage-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
CCLearner: A Deep Learning-Based Clone Detection Approach,ICSME,2017,Journal,Clone Detection,Millions,Code,Methods,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,FNN,Grid Search,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,non-tvt,non-repro-embed,repro-regularization,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs,ICLR,2018,Technical,Program Synthesis,Thousands,Symbolic Equations,Program,non-RepoMeta,non-I/O,One Hot,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RvNNs,Grid Search,Gradient Descent,MSE,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,non-F1,Increased Performance Over Predeccesor,EDA,Razor,no-miss-learning,data-balancing,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,over-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports,ASE,2015,Short Paper,Bug-Fixing Process,Tens,Bug Report,Class,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,AutoEncoder,non-IOVect,unsupervised,RBM,No Tunning,N/A-Learning,Unsupervised-Loss,Dual-metric,non-cover,MRR,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Compiler Fuzzing Through Deep Learning,ISSTA,2018,Technical,Software Testing,Thousands,Code,Program,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,unsupervised,RNN,Custom-Manual-Empirical,Gradient Descent,Unsupervised-Loss,Coverage & Proportions,Rate-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,Reprod,non-LearningAlg,repro-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deep API Learning,FSE,2016,Technical,Souce Code Generation,Millions,Natural,Snippets,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,Grid Search,Gradient Descent,Negative Log Likelihood,Alignment,non-cover,non-rank,alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Understanding of the Topic,no-EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,repro-embed,repro-regularization,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deep Code Search,ICSE,2018,Technical,Source Code Retrieval & Traceability,Thousands,Natural,Snippets,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,Cosine Loss,Trial-metric,Rate-Based,Dual-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,EDA,Razor,no-miss-learning,hyper-tuning,over-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deep Green: Modelling Time-Series of Software Energy Consumption,ICSME,2017,Journal,Software Energy Metrics,Tens,Time,Program,non-RepoMeta,non-I/O,MTEF,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,MRE,Coverage & Proportions,Energy or Memory,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Solving Previously Unsolvable Problems,no-EDA,Razor,no-miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,NoRepo
Deep Learning Based Feature Envy Detection,ASE,2018,Technical,Code Smells,Thousands,Code,Trial-code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,CNN,Grid Search,Gradient Descent,Binary Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Increased Automation / Efficiency,no-EDA,Razor,miss-learning,cross-validation,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,k-fold,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deep learning code fragments for code clone detection,ASE,2016,Technical,Clone Detection,Hundreds,Code,Dual-code,non-RepoMeta,non-I/O,One Hot,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,Autoencoder,Random Search,Gradient Descent,N/A-Loss,Timing,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,non-Recall,non-F1,Replacing Expertise,EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,repro-regularization,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Deep learning similarities from different representations of source code,MSR,2018,Technical,Clone Detection,Tens,Code,Quadral-code,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,unsupervised,Autoencoder,No Tunning,Gradient Descent,Unsupervised-Loss,Dual-metric,non-cover,Spearmans,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Advanced Architecture / Novelty,EDA,no-Razor,no-miss-learning,cross-validation,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,k-fold,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deep Learning Type Inference,FSE,2018,Technical,Code Comprehension,Hundreds,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,dropout,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deep Specification Mining,ISSTA,2018,Technical,Code Comprehension,Tens,Execution Traces,Class,non-RepoMeta,non-I/O,Execution Trace,non-Tokenization,non-NeuralEmb,non-IOVect,unsupervised,RNN,No Tunning,N/A-Learning,Unsupervised-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,F1,Increased Automation / Efficiency,EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,repro-regularization,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deepcoder: Learning to write programs,ICLR,2017,Technical,Program Synthesis,Thousands,Dual-data,Class,non-RepoMeta,I/O,One Hot,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,Cross-entropy,Timing,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,no-EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,repro-regularization,repro-EDA,non-ExDe,repro-Filter,NoRepo
Differentiable programs with neural libraries,ICML,2017,Technical,Program Synthesis,Thousands,PPBE,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,supervised,RNN,No Tunning,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Solving Previously Unsolvable Problems,no-EDA,Razor,miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,repro-loss,Hyper,tvt,non-repro-embed,non-repro-regul,repro-EDA,ExDe,repro-Filter,NoRepo
DRLgencert: Deep Learning-Based Automated Testing of Certificate Verification in SSL/TLS Implementations,ICSME,2018,Technical,Software Security,Thousands,Certificates,non-Code,non-RepoMeta,non-I/O,Lookup Tables,non-Tokenization,non-NeuralEmb,non-IOVect,unsupervised,DRL,No Tunning,Reward Policy,Q-Learning,Coverage & Proportions,Rate-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,no-Razor,miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,NoRepo
DSM: A Specification Mining Tool Using Recurrent Neural Network Based Language Model,FSE,2018,Short Paper,Code Comprehension,Tens,Execution Traces,Methods,non-RepoMeta,non-I/O,Prefix Tree,non-Tokenization,non-NeuralEmb,non-IOVect,unsupervised,RNN,Custom-Manual-Empirical,N/A-Learning,Unsupervised-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,F1,Increased Automation / Efficiency,no-EDA,no-Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,tvt,non-repro-embed,repro-regularization,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Dynamic Neural Program Embedding for Program Repair,ICLR,2018,Journal,Program Repair,Thousands,Execution Traces,Program,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Replacing Expertise,no-EDA,no-Razor,no-miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,repro-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,NoRepo
Exploring the use of deep learning for feature location,ICSME,2015,Short Paper,Feature Location,Tens,Natural,Snippets,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,supervised,FNN,No Tunning,Gradient Descent,N/A-Loss,Ranking,non-cover,MRR,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,repro-loss,Hyper,non-tvt,non-repro-embed,repro-regularization,repro-EDA,non-ExDe,non-Filter,NoRepo
From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation,ICSE,2018,Technical,Image2Structure,Thousands,Vision,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,supervised,Encoder-Decoder,Unknown Method,Gradient Descent,Max Log Likelihood,Dual-metric,non-cover,non-rank,alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Replacing Expertise,EDA,no-Razor,no-miss-learning,custom,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
How Well Do Change Sequences Predict Defects? Sequence Learning from Software Changes,TSE,2018,Journal,Reliability & Defect Prediction,Tens,Repo Metadata,non-Code,Tabular,non-I/O,CSSI,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,Grid Search,Gradient Descent,Cross-entropy,Dual-metric,Solved Tasks,non-rank,non-alignment,non-approx,RocOrAUC,non-Acc,Prec,Recall,F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,l-regularization,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Image-to-markup generation with coarse-to-fine attention,ICML,2017,Technical,Image2Structure,Thousands,Vision,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,CNN,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,Max Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,Razor,no-miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Improving Automatic Source Code Summarization via Deep Reinforcement Learning,ASE,2018,Technical,Source Code Summarization,Millions,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,MSE,Distance-Based,non-cover,non-rank,alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,pretraining,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Improving Neural Program Synthesis with Inferred Execution Traces,NIPS,2018,Technical,Program Synthesis,Thousands,I/O,non-Code,non-RepoMeta,I/O,Dual-process,non-Tokenization,CNN,I/O Vectors,supervised,RNN,No Tunning,Gradient Descent,Max Log Likelihood,Dual-metric,non-cover,Top K,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,no-Razor,no-miss-learning,gradient-clipping,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,ExDe,repro-Filter,NoRepo
Latent attention for if-then program synthesis,NIPS,2016,Technical,Program Synthesis,Thousands,Dual-data,Program,non-RepoMeta,I/O,One Hot,non-Tokenization,RNN,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,Razor,no-miss-learning,l-regularization,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,repro-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,NoRepo
Learn&Fuzz: Machine learning for input fuzzing,ASE,2017,Technical,Software Testing,Thousands,Natural,non-Code,non-RepoMeta,non-I/O,PDF Vector,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,Grid Search,CFGLA,Unsupervised-Loss,Coverage & Proportions,Coverage-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,no-Razor,miss-learning,hyper-tuning,over-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,NoRepo
Learning a dual-language vector space for domain-specific cross-lingual question retrieval,ASE,2016,Technical,Developer Forum Analysis,Thousands,Natural,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,CNN,Grid Search,Gradient Descent,MSE,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,no-EDA,Razor,no-miss-learning,l-regularization,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Learning Libraries of Subroutines for Neurally–Guided Bayesian Program Induction,NIPS,2018,Journal,Program Synthesis,Hundreds,I/O,non-Code,non-RepoMeta,I/O,I/O Vectors,non-Tokenization,non-NeuralEmb,I/O Vectors,supervised,RNN,No Tunning,Gradient Descent,Cross-entropy,Timing,Solved Tasks,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Solving Previously Unsolvable Problems,no-EDA,Razor,no-miss-learning,cleaning,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,repro-loss,Hyper,non-tvt,repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,NoRepo
Learning loop invariants for program verification,NIPS,2018,Journal,Software Testing,Hundreds,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Dual-token,non-NeuralEmb,non-IOVect,reinforcement,GNN,Custom-Manual-Empirical,Reward Policy,Actor Critic,Coverage & Proportions,Solved Tasks,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Solving Previously Unsolvable Problems,no-EDA,Razor,no-miss-learning,pretraining,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,repro-EDA,ExDe,repro-Filter,non-NoRepo
Learning program embeddings to propagate feedback on student code,ICML,2015,Technical,Code Comprehension,Thousands,Code,Snippets,non-RepoMeta,non-I/O,Execution Trace,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,Random Search,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Replacing Expertise,EDA,Razor,no-miss-learning,l-regularization,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Learning to align the source code to the compiled object code,ICML,2017,Technical,Code Comprehension,Thousands,Code,Snippets,non-RepoMeta,non-I/O,Dual-process,Raw Code,RNN,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,Negative Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,no-Razor,no-miss-learning,data-augmentation,non-o-tuning,non-o-custom,data-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Learning to infer graphics programs from hand-drawn images,NIPS,2018,Journal,Program Synthesis,Thousands,Vision,non-Code,non-RepoMeta,non-I/O,I/O Vectors,non-Tokenization,non-NeuralEmb,I/O Vectors,supervised,CNN,No Tunning,Gradient Ascent,Surrogate Likelihood,Timing,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,no-Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,repro-embed,repro-regularization,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Learning to Predict Severity of Software Vulnerability Using Only Vulnerability Description,ICSME,2017,Journal,Software Security,Thousands,Natural,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,CNN,Grid Search,Gradient Descent,Hinge Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,F1,Replacing Expertise,EDA,no-Razor,no-miss-learning,over-dual,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,over-regularization,non-o-large,replicable,non-Reprod,LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Learning to repair software vulnerabilities with generative adversarial networks,NIPS,2018,Journal,Program Repair,Thousands,Code,Dual-code,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,unsupervised,GANs,Custom-Manual-Empirical,Gradient Descent,Unsupervised-Loss,Dual-metric,non-cover,non-rank,alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Replacing Expertise,no-EDA,Razor,no-miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Learning to represent programs with graphs,ICLR,2018,Journal,Code Comprehension,Tens,Code,Program,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,GNN,No Tunning,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Replacing Expertise,EDA,Razor,miss-learning,dropout,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,ExDe,non-Filter,non-NoRepo
Leveraging grammar and reinforcement learning for neural program synthesis,ICLR,2018,Technical,Program Synthesis,Thousands,Karel DSL,non-Code,non-RepoMeta,I/O,Dual-process,non-Tokenization,CNN,I/O Vectors,reinforcement,RNN,Grid Search,Gradient Descent,Max Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,Razor,no-miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,repro-EDA,ExDe,repro-Filter,NoRepo
Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps,TSE,2018,Journal,Image2Structure,Hundreds,Vision,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,CNN,non-IOVect,supervised,CNN,Custom-Manual-Empirical,Gradient Descent,MSE,Classification,non-cover,Top K,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,data-augmentation,non-o-tuning,non-o-custom,data-augmentation,over-balancing,non-cross-val,early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Making neural programming architectures generalize via recursion,ICLR,2017,Journal,Program Synthesis,Hundreds,Execution Traces,non-Code,non-RepoMeta,non-I/O,Execution Trace,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,no-Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,repro-embed,repro-regularization,repro-EDA,non-ExDe,repro-Filter,NoRepo
Maybe Deep Neural Networks are the Best Choice for Modeling Source Code,ArXiv,2019,Technical,Souce Code Generation,Thousands,Code,Snippets,non-RepoMeta,non-I/O,BPE,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,Grid Search,Gradient Descent,Cross-entropy,Ranking,non-cover,MRR,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Open Vocabulary Issue,EDA,Razor,no-miss-learning,over-dual,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,over-cleaning,over-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing,NIPS,2018,Journal,Program Synthesis,Thousands,Natural,non-Code,non-RepoMeta,non-I/O,Directed Graph,Natural Language,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,Unknown Method,Reward Policy,MAPO,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,no-Razor,no-miss-learning,over-dual,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,ExDe,repro-Filter,non-NoRepo
Mining Likely Analogical APIs across Third-Party Libraries via Large-Scale Unsupervised API Semantics Embedding,TSE,2019,Journal,Source Code Retrieval & Traceability,Thousands,Dual-data,Program,Tabular,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,unsupervised,Encoder-Decoder,Grid Search,Gradient Descent,Unsupervised-Loss,Dual-metric,non-cover,MRR,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,Recall,non-F1,Replacing Expertise,EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,Reprod,LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,repro-regularization,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Neural Code Comprehension: A Learnable Representation of Code Semantics,NIPS,2018,Journal,Code Comprehension,Hundreds,Flow Graphs,Class,non-RepoMeta,non-I/O,Flow Graphs,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Understanding of the Topic,EDA,Razor,no-miss-learning,data-augmentation,non-o-tuning,non-o-custom,data-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Neural guided constraint logic programming for program synthesis,NIPS,2018,Technical,Program Synthesis,Thousands,I/O,non-Code,non-RepoMeta,I/O,Dual-process,non-Tokenization,RNN,I/O Vectors,supervised,GNN,Grid Search,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Neural program metainduction.,NIPS,2017,Technical,Program Synthesis,Thousands,Karel DSL,non-Code,non-RepoMeta,non-I/O,Execution Trace,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,Grid Search,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,no-EDA,Razor,no-miss-learning,cleaning,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Neural Program Synthesis from Diverse Demonstration Videos,ICML,2018,Technical,Program Synthesis,Thousands,Vision,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,Binary Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Replacing Expertise,no-EDA,no-Razor,miss-learning,cleaning,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Neural programmer-interpreters,ICLR,2015,Technical,Program Synthesis,Tens,Execution Traces,non-Code,non-RepoMeta,non-I/O,Execution Trace,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,Gradient Ascent,Max Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Solving Previously Unsolvable Problems,no-EDA,Razor,miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Neural sketch learning for conditional program generation,ICLR,2018,Journal,Program Synthesis,Thousands,Code,Dual-code,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,Cross-entropy,Distance-Based,non-cover,non-rank,non-alignment,approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,EDA,no-Razor,no-miss-learning,cross-validation,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,hold-out,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Neural-augmented Static Analysis of Android Communication,FSE,2018,Journal,Software Security,Thousands,Code,Analytics-Graphical,non-RepoMeta,non-I/O,ic3,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,TDE,No Tunning,Gradient Descent,Cross-entropy,Dual-metric,non-cover,Kruskal,non-alignment,non-approx,RocOrAUC,non-Acc,non-Prec,non-Recall,F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,data-balancing,non-o-tuning,non-o-custom,non-augmentation,over-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Neural-guided deductive search for real-time program synthesis from examples,ICLR,2018,Technical,Program Synthesis,Thousands,I/O,non-Code,non-RepoMeta,I/O,Dual-process,non-Tokenization,RNN,I/O Vectors,supervised,RNN,No Tunning,Gradient Descent,Max Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,Razor,miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,ExDe,repro-Filter,NoRepo
Neuro-Symbolic Program Corrector for Introductory Programming Assignments,ICSE,2018,Journal,Program Repair,Thousands,Code,Snippets,non-RepoMeta,non-I/O,One Hot,Raw Code,non-NeuralEmb,non-IOVect,supervised,RNN,Grid Search,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Replacing Expertise,EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,repro-loss,non-Hyper,tvt,non-repro-embed,repro-regularization,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
On the Naturalness of Proofs,FSE,2018,Short Paper,Program Synthesis,Thousands,Code,Analytics-Graphical,non-RepoMeta,non-I/O,Tokenization,Natural Language,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,Negative Log Likelihood,Distance-Based,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Understanding of the Topic,EDA,Razor,no-miss-learning,batch-norm,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Oreo: Detection of Clones in the Twilight Zone,FSE,2018,Journal,Clone Detection,Thousands,Code,Analytics-Graphical,non-RepoMeta,non-I/O,Code Metrics Vectors,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,Siamese,Grid Search,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,RocOrAUC,Acc,Prec,Recall,non-F1,Increased Automation / Efficiency,no-EDA,Razor,miss-learning,dropout,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Predicting and Evaluating Software Model Growth in the Automotive Industry,ICSME,2017,Technical,non-Code Related,Tens,Repo Metadata,non-Code,Tabular,non-I/O,Code Metrics Vectors,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,N/A-Learning,N/A-Loss,Approx Error,non-cover,non-rank,non-alignment,approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,tvt,repro-embed,repro-regularization,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Predicting Delivery Capability in Iterative Software Development,TSE,2017,Journal,non-Code Related,Thousands,Repo Metadata,non-Code,Issues,non-I/O,Issue Report Features,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,FNN,No Tunning,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Advanced Architecture / Novelty,EDA,Razor,miss-learning,dropout,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,non-Reprod,LearningAlg,repro-loss,Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Predicting semantically linkable knowledge in developer online forums via convolutional neural network,ASE,2016,Technical,Developer Forum Analysis,Thousands,Natural,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,CNN,No Tunning,Gradient Descent,Cosine Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,l-regularization,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,replicable,non-Reprod,LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,NoRepo
Predicting the delay of issues with due dates in software projects,EMSE,2017,Journal,non-Code Related,Thousands,Repo Metadata,non-Code,Issues,non-I/O,Issue Report Features,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,FNN,No Tunning,Gradient Descent,N/A-Loss,Dual-metric,non-cover,non-rank,non-alignment,approx,RocOrAUC,non-Acc,Prec,Recall,F1,Increased Automation / Efficiency,EDA,Razor,miss-learning,dropout,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Robustfill: Neural program learning under noisy i/o,ICML,2017,Technical,Program Synthesis,Millions,I/O,non-Code,non-RepoMeta,I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,supervised,RNN,Unknown Method,Gradient Descent,Max Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Semantically Enhanced Software Traceability Using Deep Learning Techniques,ICSE,2017,Technical,Source Code Retrieval & Traceability,Thousands,Natural,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,RNN,Grid Search,Gradient Descent,Negative Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,non-F1,Advanced Architecture / Novelty,EDA,Razor,miss-learning,l-regularization,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,ExDe,repro-Filter,NoRepo
Sentiment Analysis for Software Engineering: How Far Can We Go?,ICSE,2018,Journal,Developer Forum Analysis,Thousands,Repo Metadata,non-Code,Tabular,non-I/O,Tokenization,Natural Language,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,non-F1,Increased Understanding of the Topic,EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,non-tvt,repro-embed,repro-regularization,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Suggesting Accurate Method and Class Names,FSE,2015,Technical,Code Smells,Hundreds,Code,Dual-code,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,Log-Bilinear,Bayesian Opt,Gradient Descent,NCE,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,dropout,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Toward Deep Learning Software Repositories,MSR,2015,Technical,Souce Code Generation,Hundreds,Code,Dual-code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,supervised,RNN,Grid Search,Gradient Descent,Negative Log Likelihood,Distance-Based,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,repro-regularization,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques,ICSME,2017,Technical,Bug-Fixing Process,Thousands,Bug Report,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,RNN,non-IOVect,semi-supervised,Siamese,Unknown Method,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,Prec,Recall,non-F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,repro-regularization,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Towards synthesizing complex programs from input-output,ICLR,2018,Technical,Program Synthesis,Thousands,I/O,non-Code,non-RepoMeta,I/O,Lookup Tables,non-Tokenization,non-NeuralEmb,I/O Vectors,reinforcement,RNN,No Tunning,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,EDA,Razor,miss-learning,custom,non-o-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Tree-to-tree neural networks for program translation,NIPS,2018,Journal,Program Translation,Thousands,Code,Dual-code,non-RepoMeta,non-I/O,Trees,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,Grid Search,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,non-tvt,non-repro-embed,repro-regularization,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
VulSeeker: A Semantic Learning Based Vulnerability Seeker for Cross-platform Binary,ASE,2018,Short Paper,Clone Detection,Thousands,Code,Binary,non-RepoMeta,non-I/O,IDA Pro,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,FNN,No Tunning,Gradient Descent,Cosine Loss,Classification,non-cover,non-rank,non-alignment,non-approx,RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,-,no-Razor,no-miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,repro-regularization,non-repro-EDA,ExDe,repro-Filter,NoRepo
?Diff: cross-version binary code similarity detection with DNN,ASE,2018,Journal,Clone Detection,Thousands,Code,Binary,non-RepoMeta,non-I/O,IDA Pro,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,Siamese,Grid Search,Gradient Descent,Constrastive Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,Recall,non-F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,batch-norm,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,NoRepo
Mining Fix Patterns for FindBugs Violations,TSE,2018,Journal,Program Repair,Hundreds,Dual-data,Snippets,Tabular,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,unsupervised,CNN,Custom-Manual-Empirical,Gradient Descent,Unsupervised-Loss,Ranking,non-cover,Top K,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,no-Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
DeepPerf: Performance Prediction for Configurable Software with Deep Sparse Neural Network,ICSE,2019,Technical,Software Resource Control,Tens,Code,Dual-code,non-RepoMeta,non-I/O,SRCIV,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,FNN,Grid Search,Gradient Descent,N/A-Loss,Timing,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,over-dual,over-tuning,non-o-custom,non-augmentation,non-o-balancing,hold-out,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Learning to Spot and Refactor Inconsistent Method Names,ICSE,2019,Technical,Code Smells,Millions,Code,Methods,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,supervised,CNN,No Tunning,Gradient Descent,MAE,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Increased Performance Over Predeccesor,EDA,-,no-miss-learning,cleaning,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,over-cleaning,non-dropout,non-regularization,over-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
On Learning Meaningful Code Changes Via Neural Machine Translation,ICSE,2019,Technical,Code Smells,Thousands,Code,Methods,non-RepoMeta,non-I/O,Tokenization,Dual-token,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,Grid Search,Gradient Descent,Negative Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,over-quintuple,over-tuning,non-o-custom,data-augmentation,non-o-balancing,hold-out,early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
NL2Type: Inferring JavaScript Function Types from Natural Language Information,ICSE,2019,Technical,Code Smells,Thousands,Code,Methods,non-RepoMeta,non-I/O,Tokenization,Dual-token,non-NeuralEmb,non-IOVect,supervised,RNN,No Tunning,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Increased Performance Over Predeccesor,EDA,-,no-miss-learning,over-trial,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,early-stop,over-cleaning,over-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
ActionNet: Vision-Based Workflow Action Recognition From Programming Screencasts,ICSE,2019,Technical,Code Extraction From Media,Thousands,Vision,non-Code,non-RepoMeta,non-I/O,Video Frame,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,Siamese,No Tunning,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Increased Automation / Efficiency,EDA,-,miss-learning,large dataset,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,over-large,non-replicable,non-Reprod,LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Code completion with statistical language models,PLDI,2014,Technical,Souce Code Generation,Millions,Code,Analytics-Graphical,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,unsupervised,RNN,Custom-Manual-Empirical,N/A-Learning,N/A-Loss,Ranking,non-cover,Top K,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,EDA,Razor,miss-learning,hyper-tuning,over-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,over-large,non-replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Cross-language learning for program classification using bilateral tree-based convolutional neural networks,AAAI,2018,Short Paper,Software Categorization,Thousands,Code,Analytics-Graphical,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,CNN,No Tunning,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Increased Automation / Efficiency,no-EDA,no-Razor,miss-learning,pretraining,non-o-tuning,non-o-custom,non-augmentation,over-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Learning to mine aligned code and natural language pairs from stack overflow,MSR,2018,Technical,Source Code Summarization,Thousands,Natural,Snippets,non-RepoMeta,non-I/O,Tokenization,Dual-token,non-NeuralEmb,non-IOVect,unsupervised,Encoder-Decoder,No Tunning,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,RocOrAUC,non-Acc,Prec,Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,miss-learning,over-trial,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,k-fold,non-early-stop,over-cleaning,over-dropout,non-regularization,non-o-large,non-replicable,Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deep Code Comment Generation,ICPC,2018,Technical,Source Code Summarization,Thousands,Natural,Dual-code,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,N/A-Loss,Alignment,non-cover,non-rank,alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Performance Over Predeccesor,EDA,Razor,miss-learning,gradient-clipping,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,hold-out,non-early-stop,over-cleaning,over-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
DeepBugs: A learning approach to name-based bug detection,OOPSLA,2018,Technical,Bug-Fixing Process,Thousands,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,supervised,FNN,No Tunning,Gradient Descent,Binary Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,over-trial,non-o-tuning,non-o-custom,non-augmentation,over-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,over-large,replicable,non-Reprod,LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Improving bug detection via context-based code representation learning and attention-based neural networks,OOPSLA,2019,Journal,Bug-Fixing Process,Millions,Code,Dual-code,non-RepoMeta,non-I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,CNN,Custom-Manual-Empirical,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Increased Automation / Efficiency,no-EDA,Razor,miss-learning,over-dual,over-tuning,non-o-custom,non-augmentation,non-o-balancing,k-fold,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
AutoPandas: neural-backed generators for program synthesis,OOPSLA,2019,Journal,Program Synthesis,Millions,I/O,non-Code,non-RepoMeta,I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,GNN,No Tunning,Gradient Descent,Cross-entropy,Coverage & Proportions,Solved Tasks,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,no-Razor,no-miss-learning,large dataset,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,over-large,non-replicable,non-Reprod,LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,non-NoRepo
A Grammar-Based Structural CNN Decoder for Code Generation,AAAI,2019,Technical,Souce Code Generation,Tens,Natural,non-Code,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,Structure2Vec,non-IOVect,supervised,CNN,No Tunning,Gradient Descent,Cross-entropy,Dual-metric,non-cover,non-rank,alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,hyper-tuning,non-o-tuning,over-custom,non-augmentation,non-o-balancing,hold-out,non-early-stop,non-o-cleaning,over-dropout,over-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,ExDe,repro-Filter,non-NoRepo
Automatic Code Review by Learning the Revision of Source Code,AAAI,2019,Technical,Code Comprehension,Tens,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,supervised,CNN,No Tunning,Gradient Descent,Binary Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,RocOrAUC,non-Acc,non-Prec,non-Recall,F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,over-dual,non-o-tuning,over-custom,non-augmentation,non-o-balancing,k-fold,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,NoRepo
Convolutional Neural Networks over Tree Structures for Programming Language Processing,AAAI,2016,Technical,Code Comprehension,Thousands,Code,Program,non-RepoMeta,non-I/O,Neural Embedding,non-Tokenization,CNN,non-IOVect,semi-supervised,CNN,Custom-Manual-Empirical,Gradient Descent,Cross-entropy,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,Razor,no-miss-learning,pretraining,over-tuning,non-o-custom,data-augmentation,non-o-balancing,hold-out,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,non-NoRepo
Deep Reinforcement Learning for Syntactic Error Repair in Student Programs,AAAI,2019,Technical,Bug-Fixing Process,Thousands,Cursor,Methods,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,reinforcement,RNN,No Tunning,N/A-Learning,N/A-Loss,Coverage & Proportions,Dual-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,Razor,miss-learning,gradient-clipping,non-o-tuning,over-custom,non-augmentation,non-o-balancing,k-fold,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,ExDe,repro-Filter,NoRepo
DeepFix: Fixing Common C Language Errors by Deep Learning,AAAI,2017,Technical,Bug-Fixing Process,Thousands,Code,Program,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,reinforcement,Encoder-Decoder,Custom-Manual-Empirical,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,no-Razor,no-miss-learning,gradient-clipping,over-tuning,non-o-custom,non-augmentation,non-o-balancing,k-fold,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing,AAAI,2019,Technical,Software Testing,Thousands,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Natural Language,non-NeuralEmb,non-IOVect,reinforcement,Encoder-Decoder,Custom-Manual-Empirical,N/A-Learning,N/A-Loss,Coverage & Proportions,Dual-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,no-EDA,Razor,miss-learning,over-dual,over-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,over-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,repro-loss,non-Hyper,tvt,repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
DeepSIM: Deep Learning Code Functional Similarity,FSE,2018,Technical,Clone Detection,Millions,Code,Dual-code,non-RepoMeta,non-I/O,Intermediate Repr,Raw Code,non-NeuralEmb,non-IOVect,reinforcement,FNN,Custom-Manual-Empirical,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Increased Automation / Efficiency,EDA,Razor,no-miss-learning,over-sixtupel ,over-tuning,non-o-custom,non-augmentation,over-balancing,k-fold,non-early-stop,non-o-cleaning,over-dropout,over-regularization,over-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Learning to Represent Edits,ICLR,2019,Technical,Bug-Fixing Process,Thousands,Code,Snippets,non-RepoMeta,non-I/O,Tokenization,Dual-token,non-NeuralEmb,non-IOVect,unsupervised,Dual-arch,No Tunning,Gradient Descent,Negative Log Likelihood,Dual-metric,Rate-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,Acc,non-Prec,Recall,non-F1,Increased Automation / Efficiency,EDA,Razor,miss-learning,over-trial,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,hold-out,early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Neural Detection of Semantic Code Clones Via Tree-Based Convolution,ICPC,2019,Technical,Clone Detection,Thousands,Code,Dual-code,non-RepoMeta,non-I/O,Tokenization,Dual-token,non-NeuralEmb,non-IOVect,supervised,CNN,No Tunning,Gradient Descent,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Advanced Architecture / Novelty,no-EDA,Razor,no-miss-learning,over-dual,non-o-tuning,non-o-custom,non-augmentation,over-balancing,hold-out,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,non-Filter,NoRepo
A Novel Neural Source Code Representation Based on Abstract Syntax Tree,ICSE,2019,Technical,Clone Detection,Thousands,Code,Analytics-Graphical,non-RepoMeta,non-I/O,Trees,Raw Code,non-NeuralEmb,non-IOVect,supervised,RvNNs,No Tunning,Gradient Descent,Negative Log Likelihood,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,F1,Advanced Architecture / Novelty,EDA,Razor,no-miss-learning,over-dual,non-o-tuning,non-o-custom,non-augmentation,over-balancing,hold-out,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,repro-Filter,non-NoRepo
A Neural Model for Generating Natural Language Summaries of Program Subroutines,ICSE,2019,Technical,Source Code Summarization,Millions,Natural,Dual-code,non-RepoMeta,non-I/O,Tokenization,Dual-token,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,N/A-Loss,Alignment,non-cover,non-rank,alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,EDA,Razor,miss-learning,ensemble,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,hold-out,non-early-stop,over-cleaning,non-dropout,non-regularization,over-large,replicable,non-Reprod,LearningAlg,repro-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,non-NoRepo
Neuro-Symboilc Program Synthesis,ICLR,2017,Technical,Program Synthesis,Tens,I/O,non-Code,non-RepoMeta,I/O,Tokenization,Abstracted-AST-CFG,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,Custom-Manual-Empirical,Gradient Descent,N/A-Loss,Coverage & Proportions,Coverage-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,Razor,miss-learning,hyper-tuning,over-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,NoRepo
Training Binary Classifiers as Data Structure Invariants,ICSE,2019,Technical,Code Comprehension,Millions,Code,Analytics-Graphical,non-RepoMeta,non-I/O,Data Stru Field ,non-Tokenization,non-NeuralEmb,non-IOVect,supervised,FNN,Random Search,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,Prec,Recall,non-F1,Advanced Architecture / Novelty,no-EDA,no-Razor,miss-learning,over-trial,over-tuning,over-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,over-large,non-replicable,non-Reprod,LearningAlg,repro-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,repro-EDA,non-ExDe,repro-Filter,non-NoRepo
Graph Embedding Based Familial Analysis of Android Malware using Unsupervised Learning,ICSE,2019,Technical,Software Security,Thousands,Code,Analytics-Graphical,non-RepoMeta,non-I/O,Call Graph,non-Tokenization,non-NeuralEmb,non-IOVect,unsupervised,FNN,No Tunning,N/A-Learning,N/A-Loss,Classification,non-cover,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,no-EDA,Razor,miss-learning,not-discussed,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,repro-regularization,repro-EDA,non-ExDe,repro-Filter,NoRepo
DeepRoad: GAN-based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems,ASE,2018,Technical,Software Testing,Thousands,Vision,non-Code,non-RepoMeta,non-I/O,Video Frame,non-Tokenization,non-NeuralEmb,non-IOVect,unsupervised,GANs,No Tunning,Gradient Descent,Unsupervised-Loss,Coverage & Proportions,Coverage-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,no-Razor,no-miss-learning,l-regularization,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,non-o-cleaning,non-dropout,over-regularization,non-o-large,non-replicable,non-Reprod,non-LearningAlg,non-loss,Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
SeqFuzzer: An Industrial Protocol Fuzzing Framework from a Deep Learning Perspective,ICST,2019,Technical,Software Testing,Millions,Natural,non-Code,non-RepoMeta,non-I/O,Tokenization,Natural Language,non-NeuralEmb,non-IOVect,supervised,Encoder-Decoder,No Tunning,Gradient Descent,N/A-Loss,Coverage & Proportions,Rate-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Increased Automation / Efficiency,EDA,no-Razor,miss-learning,pretraining,non-o-tuning,non-o-custom,non-augmentation,non-o-balancing,hold-out,non-early-stop,non-o-cleaning,non-dropout,non-regularization,non-o-large,non-replicable,non-Reprod,LearningAlg,non-loss,non-Hyper,tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
Deep Semantic Feature Learning for Software Defect Prediction,TSE,2018,Journal,Reliability & Defect Prediction,Tens,Code,Trial-code,non-RepoMeta,non-I/O,Tokenization,Raw Code,non-NeuralEmb,non-IOVect,supervised,Deep Belief Networks,Custom-Manual-Empirical,Gradient Descent,Max Log Likelihood,Coverage & Proportions,Rate-Based,non-rank,non-alignment,non-approx,non-RocOrAUC,non-Acc,non-Prec,non-Recall,non-F1,Advanced Architecture / Novelty,EDA,Razor,no-miss-learning,over-dual,over-tuning,non-o-custom,non-augmentation,non-o-balancing,non-cross-val,non-early-stop,over-cleaning,non-dropout,non-regularization,non-o-large,replicable,non-Reprod,non-LearningAlg,non-loss,non-Hyper,non-tvt,non-repro-embed,non-repro-regul,non-repro-EDA,non-ExDe,non-Filter,NoRepo
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
